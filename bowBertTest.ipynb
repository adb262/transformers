{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##This is a notebook to demonstrate using bag of words BERT on base data"
      ],
      "metadata": {
        "id": "PQZy6i9Tvjpo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxs270QdpSvv"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/adb262/transformers.git\n",
        "%cd transformers\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "from bowBERT import BOWBERT\n",
        "from functools import partial\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "id": "FAECtcossA6j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import basic data, create iterators, vocab"
      ],
      "metadata": {
        "id": "ZiKVUjNzvUmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')"
      ],
      "metadata": {
        "id": "8Dfk17pSrsJA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "  \n",
        "pad_token_idx = 2\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<cls>\", \"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "J9D34ozApWXD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(\"<cls> \" + x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "metadata": {
        "id": "j0eSd3GcpgcN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Establish necessary variables, create model"
      ],
      "metadata": {
        "id": "KVgd89Udvd5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "LEARNING_RATE = 0.0003\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BOWBERT(vocab_size=vocab_size, n_encoders=2, d_model=64, n_heads=8, out_size=num_class, pad_token_idx=pad_token_idx).to(device)"
      ],
      "metadata": {
        "id": "iUOQ03Adp3UE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    \"\"\"Collate function for dataloaders.\"\"\"\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=pad_token_idx)\n",
        "    return label_list.to(device), text_list.to(device)"
      ],
      "metadata": {
        "id": "BhcTzbGxp4e0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Declare hyperparameters, dataloaders, and train"
      ],
      "metadata": {
        "id": "0LzittaBvCwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = partial(torch.optim.Adam, lr=LEARNING_RATE)(model.parameters())\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "accuracy = []\n",
        "log_interval = 10\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        one_hot_labels = nn.functional.one_hot(label, num_class).float()\n",
        "        predicted_label = model(text)\n",
        "        loss = criterion(predicted_label, one_hot_labels)\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            accuracy.append(total_acc/total_count)\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            predicted_label = model(text)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "metadata": {
        "id": "1Av-MIOoqez4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "total_accu = None\n",
        "\n",
        "num_train = int(len(train_dataset) * 0.90)\n",
        "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      # scheduler.step()\n",
        "      pass\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "id": "GMcTU0VBrHUn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}